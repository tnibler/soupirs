{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Utilisation avec JupyterLite (version web)\n",
    "\n",
    " - Après avoir ouvert le notebook (donc si tu vois ça), il faut attendre un peu que tout démarre bien\n",
    " - En haut à droite, il y a un petit rond à côté de \"Python (Pyodide)\", qui peut être remplit en gris ou avec un petit éclair. En glissant le curseur dessus, il y a \"Kernel status: Unknown/Idle/Busy\". Attendre que le rond ne soit plus remplit et \"Kernel status: Idle\" soit écrit.\n",
    " - Maintenant, aller sur Run > Run all cells, et l'interface avec les boutons devrait s'afficher tout en bas.\n",
    " - Au cas ou quelque chose ne marche pas, cliquer sur la petite flèche en boucle (\"Restart the kernel\"), vérifier que le bouton indique bien \"Python (Pyodide)\" (si c'est \"No Kernel\", cliquer dessus, sélectionner \"Python (Pyodide)\" et confirmer) puis attendre que le rond ne soit plus remplit, puis réessayer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Si ce message est affiche, c'est qu'on execute au moins du code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl nbformat plotly dash widgetsnbextension~=4.0.10 ipywidgets==8.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import cumulative_trapezoid\n",
    "from scipy.signal import find_peaks\n",
    "from scipy import signal\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from dash import Dash, dcc, html, Input, Output, callback\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from ipywidgets import widgets\n",
    "from dataclasses import dataclass\n",
    "from typing import TextIO\n",
    "import io\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TestFile:\n",
    "    excel_file: bytes | None\n",
    "    excel_path: Path\n",
    "    raw_file: bytes | None\n",
    "    raw_path: Path\n",
    "    label: str\n",
    "\n",
    "def list_test_file_pairs(data_dir: Path) -> list[TestFile]:\n",
    "    # Todo update this logic for files with name as prefix\n",
    "    files: list[TestFile] = []\n",
    "    for xls in data_dir.glob('*.xlsx'):\n",
    "        prefix = xls.name.split('_')[0]\n",
    "        log_match = list(data_dir.glob(f'{prefix}*raw.log'))\n",
    "        if len(log_match) == 0:\n",
    "            print(f'No matching raw log for Excel file {xls}')\n",
    "        elif len(log_match) > 1:\n",
    "            print(f'More then 1 matching raw log for Excel file {xls}?')\n",
    "        else:\n",
    "            files.append(TestFile(xls, log_match[0], label=prefix))\n",
    "    return files\n",
    "\n",
    "file_number_re = re.compile(r'\\d+_\\d{1,2}_\\d{1,2}_\\d\\d\\d\\d')\n",
    "def group_files(uploaded_files) -> list[TestFile]:\n",
    "    raw_files = dict()\n",
    "    excel_files = dict()\n",
    "    for f in uploaded_files:\n",
    "        match = file_number_re.search(f.name)\n",
    "        if match:\n",
    "            fid = match.group()\n",
    "            if f.name.endswith('.xlsx'):\n",
    "                excel_files[fid] = f\n",
    "            elif f.name.endswith('.log'):\n",
    "                raw_files[fid] = f\n",
    "            else:\n",
    "                print(f'Warning: filename {f.name} not in expected format')\n",
    "        else:\n",
    "            print(f'Warning: filename {f.name} not in expected format')\n",
    "    files = []\n",
    "    for fid, xls in excel_files.items():\n",
    "        if not fid in raw_files:\n",
    "            print(f'Missing raw log file for {xls.name}')\n",
    "            continue\n",
    "        excel_file = xls.content.tobytes()\n",
    "        rf = raw_files[fid]\n",
    "        raw_file = rf.content.tobytes()\n",
    "        files.append(TestFile(excel_file, Path(xls.name), raw_file, Path(rf.name), label=fid))\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cycle_col_names = ['phase', 'load', 'vo2/kg', 'fc', 'vo2', 'vco2', 'qr', 'vol_instant', 'bf', 've', 've/vo2', 've/vco2', 'peto2', 'petco2',\n",
    "    'vol_in', 't_in', 'vol_ex', 't_ex', 'pulse_o2', 'spo2', 'sbp', 'dbp', 'rpm', 'fo2et', 'fco2et', 'fio2et', 'fico2', 'feo2', 'feco2', 'delay_o2', 'delay_co2', 'temp_ambient',\n",
    "    'pressure_ambient', 'humidity_ambient', 'duration']\n",
    "def load_dataframes(file: TestFile, debug=False):\n",
    "    print(f'Loading files:\\n{str(file.excel_path)}\\n{str(file.raw_path)}')\n",
    "    # some xls files have empty rows at the top, so read once to know how many to skip\n",
    "    df_cycles = pd.read_excel(io.BytesIO(file.excel_file), header=None)\n",
    "\n",
    "    drop_cols = [1]\n",
    "    keep_cols = [col for col in range(len(df_cycles.columns)) if col not in drop_cols]\n",
    "    assert len(keep_cols) == len(cycle_col_names)\n",
    "    # drop all columns except indices in keep_cols\n",
    "    df_cycles.drop([df_cycles.columns[col] for col in drop_cols], axis=1, inplace=True)\n",
    "    df_cycles.columns = cycle_col_names\n",
    "    df_cycles['phase'] = df_cycles['phase'].replace('Repos', 'rest')\n",
    "    df_cycles['phase'] = df_cycles['phase'].replace('Charge', 'load')\n",
    "    df_cycles['phase'] = df_cycles['phase'].replace('Récupération', 'recovery')\n",
    "    df_cycles['phase'] = df_cycles['phase'].ffill()\n",
    "    # for all columns except col 0, cast to float and set string data in headers to NaN\n",
    "    for col in df_cycles.columns[1:]:\n",
    "        df_cycles[col] = pd.to_numeric(df_cycles[col], errors='coerce')\n",
    "    first_valid_row = 0\n",
    "    while df_cycles.loc[first_valid_row, ['vol_instant', 'vol_in', 'vol_ex', 't_in', 't_ex', 'duration']].isna().any():\n",
    "        first_valid_row += 1\n",
    "    last_valid_row = df_cycles.count().max()\n",
    "    if debug: print(f'Dropping {first_valid_row - 1} first rows')\n",
    "    df_cycles.drop(range(first_valid_row), inplace=True)\n",
    "    df_cycles.reset_index(drop=True, inplace=True)\n",
    "    n_rows = len(df_cycles)\n",
    "    invalid_rows_at_end = 0\n",
    "    while df_cycles.loc[n_rows - invalid_rows_at_end - 1, df_cycles.columns[1:]].isna().any():\n",
    "        invalid_rows_at_end += 1\n",
    "    if debug: print(f'Dropping {invalid_rows_at_end} last rows of {n_rows}')\n",
    "    if invalid_rows_at_end:\n",
    "        df_cycles.drop(range(n_rows - invalid_rows_at_end, n_rows), inplace=True)\n",
    "\n",
    "    df_cycles.reset_index(drop=True, inplace=True)\n",
    "    if debug: print(f'{len(df_cycles)} cycle rows')\n",
    "\n",
    "    df_raw = pd.read_csv(io.BytesIO(file.raw_file), delimiter='\\t', names=['t', 'flow', 'fo2', 'fco2'])\n",
    "    first_non_zero_flow_row = 0\n",
    "    flow_thresh = 0.2\n",
    "    flow_almost_zero = df_raw[df_raw['flow'].abs() > flow_thresh]\n",
    "    if len(flow_almost_zero) > 0:\n",
    "        # start a couple of samples before flow takes >0 values\n",
    "        flow_almost_zero_cutoff = max(0, flow_almost_zero.index[0] - 300) \n",
    "        if flow_almost_zero_cutoff > 0:\n",
    "            if debug: print(f'Dropping first {flow_almost_zero_cutoff} rows with flow<{flow_thresh}')\n",
    "            df_raw.drop(range(flow_almost_zero_cutoff), inplace=True)\n",
    "    df_raw.reset_index(inplace=True, drop=True)\n",
    "    df_raw['t'] = df_raw['t'] - df_raw.loc[0, 't']\n",
    "    return df_cycles, df_raw\n",
    "\n",
    "@dataclass\n",
    "class CorrResult:\n",
    "    winstart: int\n",
    "    winend: int\n",
    "    shift: int\n",
    "    win_duration_diffs: np.ndarray\n",
    "    win_durations_cycles: np.ndarray\n",
    "    corrs: np.ndarray\n",
    "    good_match: bool\n",
    "\n",
    "def find_best_corr_window(df_cycles, durations_hires, debug=False) -> CorrResult | None:\n",
    "    want_winsize = 100\n",
    "    best_winstart, best_winend, best_mean_err, best_shift = None, None, None, None\n",
    "    good_match = False\n",
    "    winstart = min(20, len(df_cycles))\n",
    "    winend = min(winstart + want_winsize, len(df_cycles))\n",
    "\n",
    "    # often times there is a stretch of zeros a few seconds into the data, and we want to cut that off.\n",
    "    MAX_SANE_CYCLE_DURATION = 30 # seconds, max duration a cycle could possibly be\n",
    "    abnormal_long_cycles_hires = durations_hires > MAX_SANE_CYCLE_DURATION\n",
    "    normal_cycles_hires = np.logical_not(abnormal_long_cycles_hires)\n",
    "    if not normal_cycles_hires.any():\n",
    "        print(f'All hires cycle longer than {MAX_SANE_CYCLE_DURATION}, this can not be right. Aborting')\n",
    "        return None\n",
    "    # obviously at the very end of data there is very long/invalid cycles which we don't want to consider for cutting off\n",
    "    abnormal_long_cycles_hires[normal_cycles_hires.argmax() - 50:] = False\n",
    "    start_cutoff_hires = min(100, abnormal_long_cycles_hires.argmax()) if abnormal_long_cycles_hires.any() else 30 # cut off first hires samples that may have extreme values\n",
    "    if debug:\n",
    "        print(f'Cutting off {start_cutoff_hires} invalid/very long cycles at the beginning')\n",
    "    best_mean_err = None\n",
    "    while not good_match and winend <= len(df_cycles):\n",
    "        winsize = winend - winstart\n",
    "        if winsize <= 0:\n",
    "            print(f'Could not find any correlation')\n",
    "            return None\n",
    "        if len(durations_hires) < winsize:\n",
    "            print(f'Error: not enough cycles in raw data to match window of size {winsize}')\n",
    "            return None\n",
    "        if debug: print(f'Finding correlation over window {winstart}-{winend}')\n",
    "        if winsize < 30:\n",
    "            print(f'Warning: small window to find initial cycle correlation')\n",
    "        # durations_hires[i] is duration of cycle from valls[i] to valls[i+1]\n",
    "        win_durations_cycles = df_cycles.loc[range(winstart, winend), 'duration'].array\n",
    "        corrs = np.correlate(durations_hires[start_cutoff_hires:] - durations_hires.mean(), win_durations_cycles - win_durations_cycles.mean(), mode='valid')\n",
    "        shift = corrs.argmax() + start_cutoff_hires\n",
    "\n",
    "        duration_diffs = df_cycles.loc[range(winstart, winend), 'duration'].array - durations_hires[shift:shift+winsize]\n",
    "        mean_err = np.abs(duration_diffs).mean()\n",
    "        if debug: print(f'Mean cycle duration error in {winsize} cycle window: {mean_err}')\n",
    "        if best_mean_err is None or mean_err < best_mean_err:\n",
    "            best_winstart = winstart\n",
    "            best_winend = winend\n",
    "            best_mean_err = mean_err\n",
    "            best_shift = shift\n",
    "            duration_diffs_sorted = np.sort(duration_diffs)\n",
    "            # sort and throw away worst 3 values when deciding if match is good or not\n",
    "            good_match = np.abs(duration_diffs_sorted[:-3]).mean() < 0.2\n",
    "        if not good_match:\n",
    "            print(f'Bad correlation in window {winstart}-{winend}')\n",
    "        winstart += 30\n",
    "        winend = min(winstart + want_winsize, len(df_cycles))\n",
    "    assert best_winstart is not None\n",
    "    assert best_winend is not None\n",
    "    assert best_shift is not None\n",
    "    if good_match:\n",
    "        print(f'Good correlation in window {winstart}-{winend}')\n",
    "    win_durations_cycles = df_cycles.loc[range(best_winstart, best_winend), 'duration'].array\n",
    "    winsize = best_winend - best_winstart\n",
    "    win_duration_diffs = df_cycles.loc[range(best_winstart, best_winend), 'duration'].array - durations_hires[best_shift:best_shift+winsize]\n",
    "    return CorrResult(best_winstart, best_winend, best_shift, win_duration_diffs, win_durations_cycles, corrs, good_match)\n",
    "\n",
    "def match_cycles_with_raw_data(df_cycles, df_raw, output, debug = False) -> bool:\n",
    "    sampling_freq_hz = 1 / (df_raw['t'][:-1] - df_raw['t'].shift(1)[1:]).mean()\n",
    "    filter_freq_hz = 3.15*1e-2\n",
    "    df_raw['instant_vol_raw'] = cumulative_trapezoid(y=df_raw['flow'], x=df_raw['t'], initial=0) \n",
    "    sos = signal.butter(4, Wn=filter_freq_hz / sampling_freq_hz, btype='highpass', output='sos')\n",
    "    flow_filtered = signal.sosfilt(sos, df_raw['flow'])\n",
    "    df_raw['instant_vol'] = cumulative_trapezoid(y=flow_filtered, x=df_raw['t'], initial=0) \n",
    "    if debug: print(f'Sum over all instantaneous volume: {df_raw.instant_vol.sum()}')\n",
    "\n",
    "    MIN_PROMINENCE = 0.15\n",
    "    peaks, peakprops  = signal.find_peaks(df_raw['instant_vol'], prominence=MIN_PROMINENCE)\n",
    "    valls, vallprops = signal.find_peaks(-df_raw['instant_vol'], prominence=MIN_PROMINENCE)\n",
    "    if debug: print(f'Found {len(peaks)} peaks, {len(valls)} valleys')\n",
    "    # index of first peak that comes after first valley\n",
    "    first_peak_idx = np.argwhere(peaks > valls[0]).min()\n",
    "    if not (valls[-1] > peaks).all(): # there is a peak after the last valley\n",
    "        # index of last peak that comes before last valley\n",
    "        last_peak_idx = np.argwhere(valls[-1] < peaks).min()\n",
    "    else:\n",
    "        last_peak_idx = len(peaks - 1)\n",
    "    if debug: print(f'Dropping first {first_peak_idx} and last {len(peaks) - last_peak_idx} peaks')\n",
    "    peaks = peaks[first_peak_idx:last_peak_idx]\n",
    "\n",
    "    #   p   p   p   n peaks\n",
    "    #  / \\_/ \\_/ \\\n",
    "    # v   v   v   v n+1 valleys\n",
    "\n",
    "    # That makes n complete cycles\n",
    "\n",
    "    assert len(peaks) == len(valls) - 1\n",
    "    iv = df_raw['instant_vol']\n",
    "    vins = iv[peaks].array - iv[valls[:-1]].array\n",
    "    vexs = iv[peaks].array - iv[valls[1:]].array\n",
    "    if debug: print(f'Found {len(peaks)} complete cycles in 125Hz data')\n",
    "\n",
    "\n",
    "    # durations_hires[i] is duration of cycle from valls[i] to valls[i+1]\n",
    "    durations_hires = df_raw['t'][valls[1:]].array - df_raw['t'][valls[:-1]].array\n",
    "\n",
    "    corr_result = find_best_corr_window(df_cycles, durations_hires, debug)\n",
    "    if corr_result is None:\n",
    "        print(f'Error: Could not find correlation to align high-res and cycle by cycle data')\n",
    "        return False\n",
    "    shift = corr_result.shift\n",
    "    winstart, winend = corr_result.winstart, corr_result.winend\n",
    "    if not corr_result.good_match:\n",
    "        print(f'Warning: potentially bad match between cycles and 125Hz data')\n",
    "    if debug or not corr_result.good_match:\n",
    "        fig, axs = plt.subplots(2)\n",
    "        axs[0].bar(range(len(corr_result.corrs)), corr_result.corrs), shift\n",
    "        winsize = winend - winstart\n",
    "        axs[0].set_title(f'Correlation for window of {winsize} cycles (raw data)')\n",
    "\n",
    "        axs[1].plot(range(winsize), durations_hires[shift:shift+winsize], label='cycle duration 125Hz')\n",
    "        axs[1].plot(range(winsize), corr_result.win_durations_cycles, label='cycle duration')\n",
    "        # axs[1].plot(range(winsize), df_cycles.loc[range(winstart, winend), 'duration'], label='cycle duration')\n",
    "        axs[1].legend()\n",
    "        with output:\n",
    "            display(fig)\n",
    "\n",
    "    best_duration_match_idx = np.abs(corr_result.win_duration_diffs).argmin()\n",
    "\n",
    "    df_raw['cycle_index'] = pd.Series(dtype=int)\n",
    "    df_cycles['hires_tstart'] = pd.Series(dtype=float)\n",
    "    df_cycles['hires_tend'] = pd.Series(dtype=float)\n",
    "    df_cycles['hires_mismatch'] = pd.Series(dtype=bool)\n",
    "    df_cycles['hires_mismatch'] = False\n",
    "    matched_cycle_index = winstart + best_duration_match_idx\n",
    "    matched_hires_valley_idx = shift + best_duration_match_idx\n",
    "    if debug: print(f'Matched cycle {matched_cycle_index}')\n",
    "    df_cycles.loc[matched_cycle_index, 'hires_tstart'] = df_raw.loc[valls[matched_hires_valley_idx], 't']\n",
    "    # df_cycles.loc[matched_cycle_index, 'hires_tend'] = df_raw.loc[valls[matched_hires_valley_idx + 1], 't']\n",
    "\n",
    "    d = df_raw.loc[valls[matched_hires_valley_idx], 't'] - df_raw.loc[valls[matched_hires_valley_idx - 1], 't']\n",
    "    dd = df_cycles.loc[matched_cycle_index, 't_in'] + df_cycles.loc[matched_cycle_index, 't_ex']\n",
    "    df_cycles.loc[matched_cycle_index]\n",
    "    # accepted relative error (%) between durations from high-res and cycle-by-cycle data\n",
    "    MAX_DURATION_ERROR = 20 / 100\n",
    "\n",
    "    # last matched valley index. start of cycle after this one in time\n",
    "    current_valley_idx = matched_hires_valley_idx\n",
    "    # walk backwards in time, matching up cycles before matched_cycle_index\n",
    "    for cycle_idx in reversed(range(0, matched_cycle_index)):\n",
    "        cycle_tend = df_cycles.loc[cycle_idx + 1, 'hires_tstart']\n",
    "        df_cycles.loc[cycle_idx, 'hires_tend'] = cycle_tend\n",
    "        if current_valley_idx <= 0:\n",
    "            print(f'Warning: not enough cycles in raw data ({cycle_idx+1} cycles left to match up during backwards walk, but no more local minima in 125Hz data)')\n",
    "            break\n",
    "        valls_before = valls[:current_valley_idx]\n",
    "        # duration if cycle starts at a valley \n",
    "        duration_valley_start = -df_raw.loc[valls_before, 't'].array + cycle_tend\n",
    "        true_cycle_duration = df_cycles.loc[cycle_idx, 't_in'] + df_cycles.loc[cycle_idx, 't_ex']\n",
    "        best_valley_idx = np.argmin(np.abs(duration_valley_start - true_cycle_duration))\n",
    "        duration_error = (duration_valley_start[best_valley_idx] - true_cycle_duration) / duration_valley_start[best_valley_idx]\n",
    "        if np.abs(duration_error) < MAX_DURATION_ERROR:\n",
    "            df_cycles.loc[cycle_idx, 'hires_tstart'] = df_raw.loc[valls_before[best_valley_idx], 't']\n",
    "            current_valley_idx = best_valley_idx\n",
    "        else: # no valley matches cycle duration in excel data\n",
    "            # duration if cycle starts at any t\n",
    "            duration_t = -df_raw['t'].array + cycle_tend\n",
    "            best_raw_idx = np.argmin(np.abs(duration_t - true_cycle_duration))\n",
    "            while current_valley_idx >= 0 and best_raw_idx <= valls[current_valley_idx]: \n",
    "                current_valley_idx -= 1\n",
    "            # must point to last matched valley, which does not exist here. +1 b/c otherwise we lose 1 valley in valls_before = valls[:current_valley_idx] \n",
    "            current_valley_idx += 1 \n",
    "            df_cycles.loc[cycle_idx, 'hires_tstart'] = df_raw.loc[best_raw_idx, 't']\n",
    "            df_cycles.loc[cycle_idx, 'hires_mismatch'] = True\n",
    "    \n",
    "    current_valley_idx = matched_hires_valley_idx + 1\n",
    "    # walk forwards in time, matching up cycles after matched_cycle_index\n",
    "    for cycle_idx in range(matched_cycle_index, len(df_cycles)):\n",
    "        cycle_tstart = df_cycles.loc[cycle_idx - 1, 'hires_tend']\n",
    "        df_cycles.loc[cycle_idx, 'hires_tstart'] = cycle_tstart\n",
    "        if current_valley_idx > len(valls) - 1:\n",
    "            print(f'Warning: not enough cycles in raw data ({len(df_cycles)-cycle_idx-1} cycles left to match up during forwards walk, but no more local minima in 125Hz data)')\n",
    "            break\n",
    "        valls_after = valls[current_valley_idx:]\n",
    "        true_cycle_duration = df_cycles.loc[cycle_idx, 't_in'] + df_cycles.loc[cycle_idx, 't_ex']\n",
    "        # duration if cycle ends at a valley \n",
    "        duration_valley_end = df_raw.loc[valls_after, 't'].array - cycle_tstart\n",
    "        best_valley_idx = np.argmin(np.abs(duration_valley_end - true_cycle_duration))\n",
    "        duration_error = np.abs(duration_valley_end[best_valley_idx] - true_cycle_duration) / true_cycle_duration\n",
    "        if duration_error < MAX_DURATION_ERROR:\n",
    "            df_cycles.loc[cycle_idx, 'hires_tend'] = df_raw.loc[valls_after[best_valley_idx], 't']\n",
    "            current_valley_idx += best_valley_idx + 1 # best_valley_idx indexes into the slide valls_after so it's an offset on top of current_valley_idx\n",
    "            continue\n",
    "        else:\n",
    "            pass\n",
    "            # print(f'cycle {cycle_idx} duration error {duration_error}')\n",
    "        # no valley matched\n",
    "        # duration if cycle starts at any t\n",
    "        raw_after_last_matched = df_raw.loc[range(valls[current_valley_idx], len(df_raw))]\n",
    "        duration_t = raw_after_last_matched['t'] - cycle_tstart\n",
    "        best_raw_idx = (duration_t - true_cycle_duration).idxmin()\n",
    "        while current_valley_idx < len(valls) and valls[current_valley_idx] <= best_raw_idx:\n",
    "            current_valley_idx += 1\n",
    "        df_cycles.loc[cycle_idx, 'hires_tend'] = df_raw.loc[best_raw_idx, 't']\n",
    "        df_cycles.loc[cycle_idx, 'hires_mismatch'] = True\n",
    "\n",
    "    df_cycles['hires_duration'] = df_cycles['hires_tend'] - df_cycles['hires_tstart']\n",
    "    for index, cycle in df_cycles.iterrows():\n",
    "        df_raw.loc[(cycle['hires_tstart'] <= df_raw['t']) & (df_raw['t'] < cycle['hires_tend']), 'cycle_index'] = index\n",
    "    return True\n",
    "\n",
    "def find_sighs(df_cycles, window_size: int, vol='avg', center: bool=True):\n",
    "    if vol == 'avg':\n",
    "        col = 'vol_avg'\n",
    "        df_cycles['vol_avg'] = 0.5 * (df_cycles['vol_in'] + df_cycles['vol_ex'])\n",
    "    if vol == 'in': col = 'vol_in'\n",
    "    if vol == 'ex': col = 'vol_ex'\n",
    "    vol = df_cycles[col]\n",
    "    col_rolling = f'{col}_rolling_median'\n",
    "    df_cycles[col_rolling] = vol.rolling(window=window_size, center=center).median().bfill().ffill()\n",
    "    df_cycles['is_sigh'] = vol > 2 * df_cycles[col_rolling]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def on_analyze_clicked(file: TestFile, output, sigh_vol='avg', rolling_center: bool=True, debug=False):\n",
    "    global df_cycles\n",
    "    global df_raw\n",
    "    global test_file \n",
    "    test_file = file\n",
    "    df_cycles, df_raw = load_dataframes(file, debug)\n",
    "    if not match_cycles_with_raw_data(df_cycles, df_raw, output, debug):\n",
    "        return\n",
    "    find_sighs(df_cycles, window_size=15, vol=sigh_vol, center=rolling_center)\n",
    "    \n",
    "    df_raw['ts'] = pd.to_datetime(df_raw['t'], unit='s')\n",
    "    cycle_maxs = df_raw.dropna(subset='cycle_index').sort_values('instant_vol', ascending=False).drop_duplicates('cycle_index').sort_values('cycle_index')\n",
    "    cycle_maxs = cycle_maxs.join(df_cycles, on='cycle_index')\n",
    "    fig = px.line(df_raw, y=['instant_vol', 'flow'], x='ts', height=500)\n",
    "    names={'flow': 'Flow', 'instant_vol': 'Vol. Instant.'}\n",
    "    fig.for_each_trace(lambda t: t.update(name = names[t.name],\n",
    "                                      legendgroup = names[t.name]\n",
    "                                         )\n",
    "                  )\n",
    "    for tstart in df_cycles.loc[df_cycles['is_sigh'],'hires_tstart']:\n",
    "        fig.add_vline(x=pd.to_datetime(tstart, unit='s'))\n",
    "    def hover_text(df) -> str:\n",
    "        cols_labels = {\n",
    "            'vol_in': 'Vol ins',\n",
    "            'vol_ex': 'Vol exp',\n",
    "            't_in': 'Durée ins',\n",
    "            't_ex': 'Durée exp',\n",
    "            'load': 'Charge',\n",
    "        }\n",
    "        fields = '<br />'.join([f'{label}: {df[col]}' for col, label in cols_labels.items()])\n",
    "        msg = f'Cycle {int(df[\"cycle_index\"])}<br />{fields}'\n",
    "        return msg\n",
    "    cycle_maxs['hover_text'] = cycle_maxs.apply(hover_text, axis=1)\n",
    "    scatter = go.Scatter(\n",
    "            x=cycle_maxs['ts'],\n",
    "            y=cycle_maxs['instant_vol'],\n",
    "            text=cycle_maxs['hover_text'],\n",
    "            name='Cycles',\n",
    "            mode='markers',\n",
    "        )\n",
    "    fig.add_trace(scatter)\n",
    "    fig.update_layout(xaxis_tickformat='%H:%M:%S',\n",
    "        legend_title_text='',\n",
    "        xaxis_title=\"Time\",\n",
    "        yaxis_title=\"Volume (L)\")\n",
    "\n",
    "    phase_changes = list(df_cycles.loc[df_cycles['phase'].shift(1) != df_cycles['phase']].iterrows())\n",
    "    text_y = df_raw['instant_vol'].max()\n",
    "    phase_colors = {\n",
    "        'rest': 'LightGreen', \n",
    "        'recovery': 'LightGreen', \n",
    "        'load': 'LightSkyBlue'\n",
    "    }\n",
    "    phase_labels = {\n",
    "        'rest': 'Repos',\n",
    "        'recovery': 'Récupération',\n",
    "        'load': 'Charge'\n",
    "    }\n",
    "    for i, (_, row) in enumerate(phase_changes):\n",
    "        left = pd.to_datetime(row['hires_tstart'], unit='s')\n",
    "        right = pd.to_datetime(df_raw['ts'].max() if i == len(phase_changes) - 1 else phase_changes[i+1][1]['hires_tstart'], unit='s')\n",
    "        fig.add_vrect(x0=left, x1=right, fillcolor=phase_colors[row['phase']], opacity=0.3, line_width=0, layer='below')\n",
    "        fig.add_annotation(x=left, y=text_y, showarrow=False, text=phase_labels[row['phase']], xanchor='left', xshift=10)\n",
    "    sighs = df_cycles[df_cycles['is_sigh']]\n",
    "    keep_sigh_cols = [0, 1, 7, 14, 15, 16, 17, 35, 36, 38, 39]\n",
    "    sighs = sighs.drop([sighs.columns[col] for col in range(len(sighs.columns)) if col not in keep_sigh_cols], axis=1)\n",
    "    sighs.columns = ['Phase', 'Charge', 'Vol Courant', 'Vol Insp', 'T Insp', 'Vol Exp', 'T Exp', 'T start (s)', 'T end (s)', 'Durée (s)', 'Méd. courante Vol']\n",
    "    print(f'{len(sighs)} soupirs')\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    display(sighs)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "select_display = widgets.Output()\n",
    "plot_display = widgets.Output()\n",
    "select = None\n",
    "center_check = None\n",
    "vol_select = None\n",
    "def f():\n",
    "    if not select or not center_check or not vol_select:\n",
    "        return\n",
    "    plot_display.clear_output()\n",
    "    with plot_display:\n",
    "        on_analyze_clicked(select.value, plot_display, sigh_vol=vol_select.value, rolling_center=center_check.value)\n",
    "\n",
    "def on_upload_changed(inputs):\n",
    "    global select\n",
    "    global center_check\n",
    "    global vol_select\n",
    "    with select_display:\n",
    "        select_display.clear_output()\n",
    "        files = group_files(inputs['new'])\n",
    "        center_check = widgets.Checkbox(value=True, description='Center point in rolling median window')\n",
    "        select = widgets.Select(options=[(str(f.excel_path), f) for f in files])\n",
    "        vol_select = widgets.Dropdown(options=[('Exp', 'ex'), ('Insp', 'in'), ('Average', 'avg')], description='Rolling vol', value='avg')\n",
    "        button = widgets.Button(description='Analyze')\n",
    "        button.on_click(lambda button: f())\n",
    "        display(select)\n",
    "        display(center_check)\n",
    "        display(vol_select)\n",
    "        display(button)\n",
    "\n",
    "upload = widgets.FileUpload(\n",
    "    multiple=True\n",
    ")\n",
    "\n",
    "upload.observe(on_upload_changed, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(select_display)\n",
    "display(plot_display)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
