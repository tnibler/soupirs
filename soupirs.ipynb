{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly dash ipyfilechooser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import cumulative_trapezoid\n",
    "from scipy.signal import find_peaks\n",
    "from scipy import signal\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from dash import Dash, dcc, html, Input, Output, callback\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from ipywidgets import widgets\n",
    "from ipyfilechooser import FileChooser\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook_connected'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TestFile:\n",
    "    excel_path: Path\n",
    "    raw_path: Path\n",
    "    label: str\n",
    "\n",
    "def list_test_file_pairs(data_dir: Path) -> list[TestFile]:\n",
    "    # Todo update this logic for files with name as prefix\n",
    "    files: list[TestFile] = []\n",
    "    for xls in data_dir.glob('*.xlsx'):\n",
    "        prefix = xls.name.split('_')[0]\n",
    "        log_match = list(data_dir.glob(f'{prefix}*raw.log'))\n",
    "        if len(log_match) == 0:\n",
    "            print(f'No matching raw log for Excel file {xls}')\n",
    "        elif len(log_match) > 1:\n",
    "            print(f'More then 1 matching raw log for Excel file {xls}?')\n",
    "        else:\n",
    "            files.append(TestFile(xls, log_match[0], label=prefix))\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_col_names = ['phase', 'load', 'vo2/kg', 'fc', 'vo2', 'vco2', 'qr', 'vol_instant', 'bf', 've', 've/vo2', 've/vco2', 'peto2', 'petco2',\n",
    "    'vol_in', 't_in', 'vol_ex', 't_ex', 'pulse_o2', 'spo2', 'sbp', 'dbp', 'rpm', 'fo2et', 'fco2et', 'fio2et', 'fico2', 'feo2', 'feco2', 'delay_o2', 'delay_co2', 'temp_ambient',\n",
    "    'pressure_ambient', 'humidity_ambient', 'duration']\n",
    "def load_dataframes(file: TestFile, debug=False):\n",
    "    print(f'Loading files:\\n{str(file.excel_path)}\\n{str(file.raw_path)}')\n",
    "    # some xls files have empty rows at the top, so read once to know how many to skip\n",
    "    df_cycles = pd.read_excel(file.excel_path, header=None)\n",
    "\n",
    "    drop_cols = [1]\n",
    "    keep_cols = [col for col in range(len(df_cycles.columns)) if col not in drop_cols]\n",
    "    # keep_col_names = ['phase', 'vol_instant', 'vol_in', 't_in', 'vol_ex', 't_ex', 'duration']\n",
    "    assert len(keep_cols) == len(cycle_col_names)\n",
    "    # drop all columns except indices in keep_cols\n",
    "    df_cycles.drop([df_cycles.columns[col] for col in drop_cols], axis=1, inplace=True)\n",
    "    df_cycles.columns = cycle_col_names\n",
    "    # for all columns except col 0, cast to float and set string data in headers to NaN\n",
    "    for col in df_cycles.columns[1:]:\n",
    "        df_cycles[col] = pd.to_numeric(df_cycles[col], errors='coerce')\n",
    "    first_valid_row = 0\n",
    "    while df_cycles.loc[first_valid_row, ['phase', 'vol_instant', 'vol_in', 'vol_ex', 't_in', 't_ex', 'duration']].isna().any():\n",
    "        first_valid_row += 1\n",
    "    last_valid_row = df_cycles.count().max()\n",
    "    if debug: print(f'Dropping {first_valid_row - 1} first rows')\n",
    "    df_cycles.drop(range(first_valid_row), inplace=True)\n",
    "    df_cycles.reset_index(drop=True, inplace=True)\n",
    "    n_rows = len(df_cycles)\n",
    "    invalid_rows_at_end = 0\n",
    "    while df_cycles.loc[n_rows - invalid_rows_at_end - 1, df_cycles.columns[1:]].isna().any():\n",
    "        invalid_rows_at_end += 1\n",
    "    if debug: print(f'Dropping {invalid_rows_at_end} last rows of {n_rows}')\n",
    "    if invalid_rows_at_end:\n",
    "        df_cycles.drop(range(n_rows - invalid_rows_at_end, n_rows), inplace=True)\n",
    "\n",
    "    df_cycles['phase'] = df_cycles['phase'].replace('Repos', 'rest')\n",
    "    df_cycles['phase'] = df_cycles['phase'].replace('Charge', 'load')\n",
    "    df_cycles['phase'] = df_cycles['phase'].replace('Récupération', 'recovery')\n",
    "    df_cycles['phase'] = df_cycles['phase'].ffill()\n",
    "\n",
    "    df_cycles.reset_index(drop=True, inplace=True)\n",
    "    if debug: print(f'{len(df_cycles)} cycle rows')\n",
    "\n",
    "    df_raw = pd.read_csv(file.raw_path, delimiter='\\t', names=['t', 'flow', 'fo2', 'fco2'])\n",
    "    first_non_zero_flow_row = 0\n",
    "    flow_thresh = 1e-5\n",
    "    while np.abs(df_raw.loc[first_non_zero_flow_row, 'flow']) < flow_thresh:\n",
    "        first_non_zero_flow_row += 1\n",
    "    if debug: print(f'Dropping first {first_non_zero_flow_row} rows with flow<{flow_thresh}')\n",
    "    df_raw.drop(range(first_non_zero_flow_row), inplace=True)\n",
    "    df_raw.reset_index(inplace=True, drop=True)\n",
    "    return df_cycles, df_raw\n",
    "\n",
    "def match_cycles_with_raw_data(df_cycles, df_raw, debug = False):\n",
    "    sampling_freq_hz = 1 / (df_raw['t'][:-1] - df_raw['t'].shift(1)[1:]).mean()\n",
    "    filter_freq_hz = 2*1e-6\n",
    "    df_raw['instant_vol_raw'] = cumulative_trapezoid(y=df_raw['flow'], x=df_raw['t'], initial=0) \n",
    "    sos = signal.butter(4, Wn=filter_freq_hz * sampling_freq_hz, btype='highpass', output='sos')\n",
    "    flow_filtered = signal.sosfilt(sos, df_raw['flow'])\n",
    "    df_raw['instant_vol'] = cumulative_trapezoid(y=flow_filtered, x=df_raw['t'], initial=0) \n",
    "    if debug: print(f'Sum over all instantaneous volume: {df_raw.instant_vol.sum()}')\n",
    "\n",
    "    MIN_PROMINENCE = 0.15\n",
    "    peaks, peakprops  = signal.find_peaks(df_raw['instant_vol'], prominence=MIN_PROMINENCE)\n",
    "    valls, vallprops = signal.find_peaks(-df_raw['instant_vol'], prominence=MIN_PROMINENCE)\n",
    "    if debug: print(f'Found {len(peaks)} peaks, {len(valls)} valleys')\n",
    "    # index of first peak that comes after first valley\n",
    "    first_peak_idx = np.argwhere(peaks > valls[0]).min()\n",
    "    if not (valls[-1] > peaks).all(): # there is a peak after the last valley\n",
    "        # index of last peak that comes before last valley\n",
    "        last_peak_idx = np.argwhere(valls[-1] < peaks).min()\n",
    "    else:\n",
    "        last_peak_idx = len(peaks - 1)\n",
    "    if debug: print(f'Dropping first {first_peak_idx} and last {len(peaks) - last_peak_idx} peaks')\n",
    "    peaks = peaks[first_peak_idx:last_peak_idx]\n",
    "\n",
    "    #   p   p   p   n peaks\n",
    "    #  / \\_/ \\_/ \\\n",
    "    # v   v   v   v n+1 valleys\n",
    "\n",
    "    # That makes n complete cycles\n",
    "\n",
    "    assert len(peaks) == len(valls) - 1\n",
    "    iv = df_raw['instant_vol']\n",
    "    vins = iv[peaks].array - iv[valls[:-1]].array\n",
    "    vexs = iv[peaks].array - iv[valls[1:]].array\n",
    "    if debug: print(f'Found {len(peaks)} complete cycles in 125Hz data')\n",
    "\n",
    "    want_winsize = 100\n",
    "    winstart = min(100, len(df_cycles))\n",
    "    winend = min(winstart + want_winsize, len(df_cycles))\n",
    "    winsize = winend - winstart\n",
    "    if debug: print(f'Finding correlation over window {winstart}-{winend}')\n",
    "    if winsize < 50:\n",
    "        print(f'Warning: small window to find initial cycle correlation')\n",
    "    if len(valls) < winsize:\n",
    "        print(f'Error: not enough cycles in raw data to match window of size {winsize}')\n",
    "        return\n",
    "    # durations_hires[i] is duration of cycle from valls[i] to valls[i+1]\n",
    "    durations_hires = df_raw['t'][valls[1:]].array - df_raw['t'][valls[:-1]].array\n",
    "    win_durations_cycles = df_cycles.loc[range(winstart, winend), 'duration'].array\n",
    "    start_cutoff_hires = 30 # cut off first hires samples that may have extreme values\n",
    "    corrs = np.correlate(durations_hires[start_cutoff_hires:] - durations_hires.mean(), win_durations_cycles - win_durations_cycles.mean(), mode='valid')\n",
    "    shift = corrs.argmax() + start_cutoff_hires\n",
    "\n",
    "    duration_diffs = df_cycles.loc[range(winstart, winend), 'duration'].array - durations_hires[shift:shift+winsize]\n",
    "    if debug: print(f'Mean cycle duration error in {winsize} cycle window: {np.abs(duration_diffs).mean()}')\n",
    "    bad_match = np.abs(duration_diffs).mean() > 0.2\n",
    "    if bad_match:\n",
    "        print(f'Warning: potentially bad match between cycles and 125Hz data')\n",
    "    if debug or bad_match:\n",
    "        fig, axs = plt.subplots(2)\n",
    "        axs[0].bar(range(len(corrs)), corrs), shift\n",
    "        axs[0].set_title(f'Correlation for window of {winsize} cycles (raw data)')\n",
    "\n",
    "        axs[1].plot(range(winsize), durations_hires[shift:shift+winsize], label='cycle duration 125Hz')\n",
    "        axs[1].plot(range(winsize), win_durations_cycles, label='cycle duration')\n",
    "        axs[1].legend()\n",
    "\n",
    "    best_duration_match_idx = np.abs(duration_diffs).argmin()\n",
    "\n",
    "    df_raw['cycle_index'] = pd.Series(dtype=int)\n",
    "    df_cycles['hires_tstart'] = pd.Series(dtype=float)\n",
    "    df_cycles['hires_tend'] = pd.Series(dtype=float)\n",
    "    df_cycles['hires_mismatch'] = pd.Series(dtype=bool)\n",
    "    df_cycles['hires_mismatch'] = False\n",
    "    matched_cycle_index = winstart + best_duration_match_idx\n",
    "    matched_hires_valley_idx = shift + best_duration_match_idx\n",
    "    if debug: print(f'Matched cycle {matched_cycle_index}')\n",
    "    df_cycles.loc[matched_cycle_index, 'hires_tstart'] = df_raw.loc[valls[matched_hires_valley_idx], 't']\n",
    "    # df_cycles.loc[matched_cycle_index, 'hires_tend'] = df_raw.loc[valls[matched_hires_valley_idx + 1], 't']\n",
    "\n",
    "    d = df_raw.loc[valls[matched_hires_valley_idx], 't'] - df_raw.loc[valls[matched_hires_valley_idx - 1], 't']\n",
    "    dd = df_cycles.loc[matched_cycle_index, 't_in'] + df_cycles.loc[matched_cycle_index, 't_ex']\n",
    "    df_cycles.loc[matched_cycle_index]\n",
    "    # accepted relative error (%) between durations from high-res and cycle-by-cycle data\n",
    "    MAX_DURATION_ERROR = 10 / 100\n",
    "\n",
    "    # last matched valley index. start of cycle after this one in time\n",
    "    current_valley_idx = matched_hires_valley_idx\n",
    "    # walk backwards in time, matching up cycles before matched_cycle_index\n",
    "    for cycle_idx in reversed(range(0, matched_cycle_index)):\n",
    "        cycle_tend = df_cycles.loc[cycle_idx + 1, 'hires_tstart']\n",
    "        df_cycles.loc[cycle_idx, 'hires_tend'] = cycle_tend\n",
    "        if current_valley_idx <= 0:\n",
    "            print(f'Warning: not enough cycles in raw data ({cycle_idx+1} cycles left to match up during backwards walk, but no more local minima in 125Hz data)')\n",
    "            break\n",
    "        valls_before = valls[:current_valley_idx]\n",
    "        # duration if cycle starts at a valley \n",
    "        duration_valley_start = -df_raw.loc[valls_before, 't'].array + cycle_tend\n",
    "        true_cycle_duration = df_cycles.loc[cycle_idx, 't_in'] + df_cycles.loc[cycle_idx, 't_ex']\n",
    "        best_valley_idx = np.argmin(np.abs(duration_valley_start - true_cycle_duration))\n",
    "        duration_error = (duration_valley_start[best_valley_idx] - true_cycle_duration) / duration_valley_start[best_valley_idx]\n",
    "        if np.abs(duration_error) < MAX_DURATION_ERROR:\n",
    "            df_cycles.loc[cycle_idx, 'hires_tstart'] = df_raw.loc[valls_before[best_valley_idx], 't']\n",
    "            current_valley_idx = best_valley_idx\n",
    "        else: # no valley matches cycle duration in excel data\n",
    "            # duration if cycle starts at any t\n",
    "            duration_t = -df_raw['t'].array + cycle_tend\n",
    "            best_raw_idx = np.argmin(np.abs(duration_t - true_cycle_duration))\n",
    "            while best_raw_idx <= valls[current_valley_idx]: \n",
    "                current_valley_idx -= 1\n",
    "            df_cycles.loc[cycle_idx, 'hires_tstart'] = df_raw.loc[best_raw_idx, 't']\n",
    "            df_cycles.loc[cycle_idx, 'hires_mismatch'] = True\n",
    "    \n",
    "    current_valley_idx = matched_hires_valley_idx + 1\n",
    "    # walk forwards in time, matching up cycles after matched_cycle_index\n",
    "    for cycle_idx in range(matched_cycle_index, len(df_cycles)):\n",
    "        cycle_tstart = df_cycles.loc[cycle_idx - 1, 'hires_tend']\n",
    "        df_cycles.loc[cycle_idx, 'hires_tstart'] = cycle_tstart\n",
    "        if current_valley_idx > len(valls) - 1:\n",
    "            print(f'Warning: not enough cycles in raw data ({len(df_cycles)-cycle_idx-1} cycles left to match up during forwards walk, but no more local minima in 125Hz data)')\n",
    "            break\n",
    "        valls_after = valls[current_valley_idx:]\n",
    "        true_cycle_duration = df_cycles.loc[cycle_idx, 't_in'] + df_cycles.loc[cycle_idx, 't_ex']\n",
    "        # duration if cycle ends at a valley \n",
    "        duration_valley_end = df_raw.loc[valls_after, 't'].array - cycle_tstart\n",
    "        best_valley_idx = np.argmin(np.abs(duration_valley_end - true_cycle_duration))\n",
    "        duration_error = np.abs(duration_valley_end[best_valley_idx] - true_cycle_duration) / true_cycle_duration\n",
    "        if duration_error < MAX_DURATION_ERROR:\n",
    "            df_cycles.loc[cycle_idx, 'hires_tend'] = df_raw.loc[valls_after[best_valley_idx], 't']\n",
    "            current_valley_idx += best_valley_idx + 1 # best_valley_idx indexes into the slide valls_after so it's an offset on top of current_valley_idx\n",
    "            continue\n",
    "        else:\n",
    "            pass\n",
    "            # print(f'cycle {cycle_idx} duration error {duration_error}')\n",
    "        # no valley matched\n",
    "        # duration if cycle starts at any t\n",
    "        raw_after_last_matched = df_raw.loc[range(valls[current_valley_idx], len(df_raw))]\n",
    "        duration_t = raw_after_last_matched['t'] - cycle_tstart\n",
    "        best_raw_idx = (duration_t - true_cycle_duration).idxmin()\n",
    "        while current_valley_idx < len(valls) and valls[current_valley_idx] <= best_raw_idx:\n",
    "            current_valley_idx += 1\n",
    "        df_cycles.loc[cycle_idx, 'hires_tend'] = df_raw.loc[best_raw_idx, 't']\n",
    "        df_cycles.loc[cycle_idx, 'hires_mismatch'] = True\n",
    "\n",
    "    df_cycles['hires_duration'] = df_cycles['hires_tend'] - df_cycles['hires_tstart']\n",
    "    for index, cycle in df_cycles.iterrows():\n",
    "        df_raw.loc[(cycle['hires_tstart'] <= df_raw['t']) & (df_raw['t'] < cycle['hires_tend']), 'cycle_index'] = index\n",
    "\n",
    "def find_sighs(df_cycles, window_size: int):\n",
    "    vol_ex = df_cycles['vol_ex']\n",
    "    rolling_median = vol_ex.rolling(window=window_size, center=True).median().bfill().ffill()\n",
    "    df_cycles['is_sigh'] = vol_ex > 2 * rolling_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_analyze_clicked(file: TestFile, debug=False):\n",
    "    global df_cycles\n",
    "    global df_raw\n",
    "    global test_file \n",
    "    test_file = file\n",
    "    df_cycles, df_raw = load_dataframes(file, debug)\n",
    "    match_cycles_with_raw_data(df_cycles, df_raw, debug)\n",
    "    find_sighs(df_cycles, window_size=14)\n",
    "\n",
    "    cycle_maxs = df_raw.dropna(subset='cycle_index').sort_values('instant_vol', ascending=False).drop_duplicates('cycle_index').sort_values('cycle_index')\n",
    "    cycle_maxs = cycle_maxs.join(df_cycles, on='cycle_index')\n",
    "\n",
    "    fig = px.line(df_raw, y='instant_vol', x='t')\n",
    "    for tstart in df_cycles.loc[df_cycles['is_sigh'], 'hires_tstart']:\n",
    "        fig.add_vline(x=tstart)\n",
    "\n",
    "    scatter = go.Scatter(\n",
    "            x=cycle_maxs['t'],\n",
    "            y=cycle_maxs['instant_vol'],\n",
    "            name='Cycles',\n",
    "            mode='markers',\n",
    "            customdata=cycle_maxs,\n",
    "        )\n",
    "    fig.add_trace(scatter)\n",
    "    fig.update_layout(clickmode='event+select')\n",
    "\n",
    "    phase_changes = list(df_cycles.loc[df_cycles['phase'].shift(1) != df_cycles['phase']].iterrows())\n",
    "    text_y = df_raw['instant_vol'].max()\n",
    "    phase_colors = {\n",
    "        'rest': 'LightGreen', \n",
    "        'recovery': 'LightGreen', \n",
    "        'load': 'LightSkyBlue'\n",
    "    }\n",
    "    phase_labels = {\n",
    "        'rest': 'Repos',\n",
    "        'recovery': 'Récupération',\n",
    "        'load': 'Charge'\n",
    "    }\n",
    "    for i, (_, row) in enumerate(phase_changes):\n",
    "        left = row['hires_tstart']\n",
    "        right = df_raw['t'].max() if i == len(phase_changes) - 1 else phase_changes[i+1][1]['hires_tstart']\n",
    "        fig.add_vrect(x0=left, x1=right, fillcolor=phase_colors[row['phase']], opacity=0.3, line_width=0, layer='below')\n",
    "        fig.add_annotation(x=left, y=text_y, showarrow=False, text=phase_labels[row['phase']], xanchor='left', xshift=10)\n",
    "    # todo legend\n",
    "    external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "    app = Dash(\"Soupirs\", external_stylesheets=external_stylesheets)\n",
    "    app.layout = html.Div([\n",
    "        dcc.Graph(id='graph', figure=fig),\n",
    "        html.Pre(id='click-data')\n",
    "    ])\n",
    "    app.run()\n",
    "\n",
    "@callback(\n",
    "    Output('click-data', 'children'),\n",
    "    Input('graph', 'clickData'))\n",
    "def display_click_data(clickData):\n",
    "    clickdata = clickData\n",
    "    if not clickdata:\n",
    "        return None\n",
    "    if 'points' not in clickdata or len(clickdata['points']) == 0 or 'customdata' not in clickdata['points'][0]:\n",
    "        return None\n",
    "    data = clickdata['points'][0]['customdata']\n",
    "    # assert len(data) == 7 + 4 + len(cycle_col_names)\n",
    "    cols = data[7:-4]\n",
    "    # rows = [f'<tr><td>{cycle_col_names[i]}</td><td>{col}</td></tr>' for i, col in enumerate(cols)]\n",
    "    return html.Table([html.Tr([html.Td(cycle_col_names[i]), html.Td(col)]) for i, col in enumerate(cols)])\n",
    "    # return '<table>' + '\\n'.join(rows) + '</table>'\n",
    "    # return str(cols)\n",
    "\n",
    "def on_dir_chosen(chooser, out):\n",
    "    data_dir = Path(chooser.value)\n",
    "    files = list_test_file_pairs(data_dir)\n",
    "    select = widgets.Select(options=[(file.label, file) for file in files], layout={'height': '300px'})\n",
    "    debug = widgets.Checkbox(value=False, description='Enable debug output')\n",
    "    button = widgets.Button(description='Analyze')\n",
    "    button.on_click(lambda button: on_analyze_clicked(select.value, debug.value))\n",
    "    box = widgets.VBox([select, widgets.HBox([button, debug])])\n",
    "    with out:\n",
    "        display(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = FileChooser()\n",
    "fc.show_only_dirs=True\n",
    "fc.default_path = '/home/thomas/p/soupirs/data/CPET ambu anonyme'\n",
    "fc.title = 'Pick folder with raw csv and excel files'\n",
    "out = widgets.Output()\n",
    "fc.register_callback(lambda chooser: on_dir_chosen(chooser, out))\n",
    "with out:\n",
    "    display(fc)\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cycles.loc[73]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_analyze_clicked(test_file, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "*Edge cases*\n",
    "|file|time|\n",
    "|-|-|\n",
    "|ambu 1091879|836\n",
    "\n",
    "312525 is fucked\n",
    "491508\n",
    "264992\n",
    "\n",
    "### our fault\n",
    "\n",
    "518108 broken\n",
    "\n",
    "49683 breaks\n",
    "\n",
    "98442996"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fancy matching algo\n",
    "\n",
    "durations -> small window -> correlation to find anchor points to match hires time to cycle times\n",
    "\n",
    "then find all zero crossings or value close to zero of flow/derivative (points of interest) and build out cycles by taking \n",
    "the next point of interest most closely matching the excel cycle we're currently reconstucting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cycles, df_raw = load_dataframes(test_file)\n",
    "\n",
    "sampling_freq_hz = 1 / (df_raw['t'][:-1] - df_raw['t'].shift(1)[1:]).mean()\n",
    "filter_freq_hz = 2*1e-6\n",
    "df_raw['instant_vol_raw'] = cumulative_trapezoid(y=df_raw['flow'], x=df_raw['t'], initial=0) \n",
    "sos = signal.butter(4, Wn=filter_freq_hz * sampling_freq_hz, btype='highpass', output='sos')\n",
    "flow_filtered = signal.sosfilt(sos, df_raw['flow'])\n",
    "df_raw['instant_vol'] = cumulative_trapezoid(y=flow_filtered, x=df_raw['t'], initial=0) \n",
    "print(f'Sum over all instantaneous volume: {df_raw.instant_vol.sum()}')\n",
    "\n",
    "MIN_PROMINENCE = 0.10\n",
    "peaks, peakprops  = signal.find_peaks(df_raw['instant_vol'], prominence=MIN_PROMINENCE)\n",
    "valls, vallprops = signal.find_peaks(-df_raw['instant_vol'], prominence=MIN_PROMINENCE)\n",
    "print(f'Found {len(peaks)} peaks, {len(valls)} valleys')\n",
    "# index of first peak that comes after first valley\n",
    "first_peak_idx = np.argwhere(peaks > valls[0]).min()\n",
    "if not (valls[-1] > peaks).all(): # there is a peak after the last valley\n",
    "    # index of last peak that comes before last valley\n",
    "    last_peak_idx = np.argwhere(valls[-1] < peaks).min()\n",
    "else:\n",
    "    last_peak_idx = len(peaks - 1)\n",
    "print(f'Dropping first {first_peak_idx} and last {len(peaks) - last_peak_idx} peaks')\n",
    "peaks = peaks[first_peak_idx:last_peak_idx]\n",
    "\n",
    "#   p   p   p   n peaks\n",
    "#  / \\_/ \\_/ \\\n",
    "# v   v   v   v n+1 valleys\n",
    "\n",
    "# That makes n complete cycles\n",
    "\n",
    "assert len(peaks) == len(valls) - 1\n",
    "iv = df_raw['instant_vol']\n",
    "vins = iv[peaks].array - iv[valls[:-1]].array\n",
    "vexs = iv[peaks].array - iv[valls[1:]].array\n",
    "print(f'Found {len(peaks)} complete cycles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winstart = min(100, len(df_cycles))\n",
    "want_winsize = 100\n",
    "winend = min(winstart + want_winsize, len(df_cycles))\n",
    "winsize = winend - winstart\n",
    "if winsize < 50:\n",
    "    print(f'Warning: small window to find initial cycle correlation')\n",
    "if len(valls) < winsize:\n",
    "    print(f'Error: not enough cycles in raw data to match window of size {winsize}')\n",
    "durations_hires = df_raw['t'][valls[1:]].array - df_raw['t'][valls[:-1]].array\n",
    "win_durations_cycles = df_cycles.loc[range(winstart, winend), 'duration'].array\n",
    "start_cutoff_hires = 30 # cut off first hires samples that may have extreme values\n",
    "corrs = np.correlate(durations_hires[start_cutoff_hires:] - durations_hires.mean(), win_durations_cycles - win_durations_cycles.mean(), mode='valid')\n",
    "shift = corrs.argmax() + start_cutoff_hires\n",
    "plt.bar(range(len(corrs)), corrs), shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(winsize), durations_hires[shift:shift+winsize], label='duration hires')\n",
    "ax.plot(range(winsize), win_durations_cycles, label='duration cycles')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_diffs = df_cycles.loc[range(winstart, winend), 'duration'].array - durations_hires[shift:shift+winsize]\n",
    "assert np.abs(duration_diffs).max() < 0.15\n",
    "best_duration_match_idx = np.abs(duration_diffs).argmin()\n",
    "# this cycle starts at                           this time\n",
    "df_cycles.loc[winstart+best_duration_match_idx], durations_hires[shift+best_duration_match_idx], df_raw.loc[valls[shift+best_duration_match_idx]], df_raw.loc[valls[shift+best_duration_match_idx+1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cycles['hires_tstart'] = pd.Series()\n",
    "df_cycles['hires_tend'] = pd.Series()\n",
    "matched_cycle_index = winstart + best_duration_match_idx\n",
    "matched_hires_valley_idx = shift + best_duration_match_idx\n",
    "df_cycles.loc[matched_cycle_index, 'hires_tstart'] = df_raw.loc[valls[matched_hires_valley_idx], 't']\n",
    "\n",
    "flow_zeros = df_raw['flow'][df_raw['flow'].abs() < 0.001].index\n",
    "\n",
    "DURATION_ERROR = 0.1\n",
    "\n",
    "current_valley_idx = matched_hires_valley_idx\n",
    "# walk backwards in time\n",
    "for cycle_idx in reversed(range(0, matched_cycle_index)):\n",
    "    cycle_tend = df_cycles.loc[cycle_idx + 1, 'hires_tstart']\n",
    "    df_cycles.loc[cycle_idx, 'hires_tend'] = cycle_tend\n",
    "    valls_before = valls[:current_valley_idx]\n",
    "    # duration if cycle starts at a valley \n",
    "    duration_valley_start = -df_raw.loc[valls_before, 't'].array + cycle_tend\n",
    "    best_valley_idx = np.argmin(np.abs(duration_valley_start))\n",
    "    true_cycle_duration = df_cycles.loc[cycle_idx, 't_in'] + df_cycles.loc[cycle_idx, 't_ex']\n",
    "    if np.abs(duration_valley_start[best_valley_idx] - true_cycle_duration) / true_cycle_duration < DURATION_ERROR:\n",
    "        df_cycles.loc[cycle_idx, 'hires_tstart'] = df_raw.loc[valls_before[best_valley_idx], 't']\n",
    "        current_valley_idx = best_valley_idx\n",
    "    else:\n",
    "        # duration if cycle starts at any t\n",
    "        duration_t = -df_raw['t'].array + cycle_tend\n",
    "        best_raw_idx = np.argmin(np.abs(duration_t - true_cycle_duration))\n",
    "        print(f'error: {cycle_idx}, {duration_t[best_raw_idx] - true_cycle_duration}')\n",
    "        while best_raw_idx <= valls[current_valley_idx]: \n",
    "            current_valley_idx -= 1\n",
    "        df_cycles.loc[cycle_idx, 'hires_tstart'] = df_raw.loc[best_raw_idx, 't']\n",
    "\n",
    "current_valley_idx = matched_hires_valley_idx + 1\n",
    "for cycle_idx in range(matched_cycle_index, len(df_cycles)):\n",
    "    cycle_tstart = df_cycles.loc[cycle_idx - 1, 'hires_tend']\n",
    "    df_cycles.loc[cycle_idx, 'hires_tstart'] = cycle_tstart\n",
    "    valls_after = valls[current_valley_idx:]\n",
    "    true_cycle_duration = df_cycles.loc[cycle_idx, 't_in'] + df_cycles.loc[cycle_idx, 't_ex']\n",
    "    if len(valls_after) > 0:\n",
    "        # duration if cycle ends at a valley \n",
    "        duration_valley_end = df_raw.loc[valls_after, 't'].array - cycle_tstart\n",
    "        best_valley_idx = np.argmin(np.abs(duration_valley_end))\n",
    "        if np.abs(duration_valley_end[best_valley_idx] - true_cycle_duration) / true_cycle_duration < DURATION_ERROR:\n",
    "            df_cycles.loc[cycle_idx, 'hires_tend'] = df_raw.loc[valls_after[best_valley_idx], 't']\n",
    "            current_valley_idx += best_valley_idx + 1 # best_valley_idx indexes into the slide valls_after so it's an offset on top of current_valley_idx\n",
    "            continue\n",
    "        print(f'no matching valley: {cycle_idx}, {true_cycle_duration}, {duration_valley_end[best_valley_idx]}, {duration_valley_end[best_valley_idx] - true_cycle_duration}')\n",
    "    # no valley matched\n",
    "    # duration if cycle starts at any t\n",
    "    duration_t = df_raw['t'].array - cycle_tstart\n",
    "    best_raw_idx = np.argmin(np.abs(duration_t - true_cycle_duration))\n",
    "    # print(f'error: {cycle_idx}, {(duration_t[best_raw_idx] - true_cycle_duration) / true_cycle_duration}')\n",
    "    while current_valley_idx < len(valls) and best_raw_idx <= valls[current_valley_idx]: \n",
    "        current_valley_idx += 1\n",
    "    df_cycles.loc[cycle_idx, 'hires_tend'] = df_raw.loc[best_raw_idx, 't']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cycles['hires_duration'] = df_cycles['hires_tend'] - df_cycles['hires_tstart']\n",
    "df_cycles['hires_duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots() \n",
    "ax.plot(range(len(df_cycles)), df_cycles['duration'], label='duration cycles')\n",
    "ax.plot(range(len(df_cycles)), df_cycles['hires_duration'], label='duration hires')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pad_cycle_data = len(vins) - len(df_cycles['vol_in'])\n",
    "cyc_vol_in_padded = np.pad(df_cycles['vol_in'].array, (pad_cycle_data, 0))\n",
    "corr = np.correlate(cyc_vol_in_padded, vins, 'full')\n",
    "# plt.bar(range(len(corr)), corr)\n",
    "# return\n",
    "shift = corr.argmax() - (len(vins) - 1)\n",
    "assert shift >= 0\n",
    "drop_highres_cycles_front = pad_cycle_data - shift\n",
    "drop_highres_cycles_back = shift \n",
    "vins_trimmed = vins[drop_highres_cycles_front:len(vins) - drop_highres_cycles_back]\n",
    "peaks_trimmed = peaks[drop_highres_cycles_front:len(peaks) - drop_highres_cycles_back]\n",
    "valls_trimmed = valls[drop_highres_cycles_front:len(valls) - drop_highres_cycles_back]\n",
    "print(len(peaks_trimmed), len(valls_trimmed), len(df_cycles))\n",
    "assert len(peaks_trimmed) == len(df_cycles)\n",
    "assert len(valls_trimmed) == len(df_cycles) + 1\n",
    "df_cycles['highres_t_start'] = df_raw['t'][valls_trimmed[:-1]].values\n",
    "df_cycles['highres_t_max'] = df_raw['t'][peaks_trimmed].values\n",
    "df_cycles['highres_t_end'] = df_raw['t'][valls_trimmed[1:]].values\n",
    "df_cycles['highres_duration'] = df_cycles['highres_t_end'] - df_cycles['highres_t_start']\n",
    "# df_raw.set_index('t', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_cycles, y=['duration', df_cycles['highres_t_end'] - df_cycles['highres_t_start']], hover_data=['highres_t_start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['vol_cycle_start'] = pd.Series(dtype=float)\n",
    "df_raw.loc[df_raw['t'].isin(df_cycles['highres_t_start'].values), 'vol_cycle_start'] = df_raw[df_raw['t'].isin(df_cycles['highres_t_start'].values)]['instant_vol']\n",
    "fig = px.line(df_raw, x='t', y='instant_vol')\n",
    "fig = fig.add_scatter(x=df_raw.t, y=df_raw['vol_cycle_start'], mode='lines+markers')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_correlated = pd.DataFrame()\n",
    "df_correlated['vol_in_highres'] = vins_trimmed\n",
    "df_correlated['vol_in_cycles'] = df_cycles['vol_in']\n",
    "px.scatter(df_correlated, df_correlated.index, ['vol_in_highres', 'vol_in_cycles'], title='Vol Insp cycles/highres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vexs_trimmed = vexs[drop_highres_cycles_front:len(vexs)-drop_highres_cycles_back]\n",
    "cyc_vol_ex = df_cycles['vol_ex']\n",
    "df_correlated['vol_ex_highres'] = vexs_trimmed\n",
    "df_correlated['vol_ex_cycles'] = df_cycles['vol_ex']\n",
    "px.scatter(df_correlated, df_correlated.index, ['vol_ex_highres', 'vol_ex_cycles'], title='Vol Exp cycles/highres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_raw, x='t', y=['instant_vol', 'flow'])\n",
    "# px.scatter(df_raw, x='t', y=peaks)\n",
    "# px.line(df_raw, x='t', y=['flow', 'flow_ma', 'filtered', 'instant_vol'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
